%%
%% This is file `tikzposter-template.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% tikzposter.dtx  (with options: `tikzposter-template.tex')
%%
%% This is a generated file.
%%
%% Copyright (C) 2014 by Pascal Richter, Elena Botoeva, Richard Barnard, and Dirk Surmann
%%
%% This file may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either
%% version 2.0 of this license or (at your option) any later
%% version. The latest version of this license is in:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% and version 2.0 or later is part of all distributions of
%% LaTeX version 2013/12/01 or later.
%%


\documentclass{tikzposter} %Options for format can be included here

\usepackage{todonotes}

\usepackage[tikz]{bclogo}
\usepackage{lipsum}
\usepackage{amsmath}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[absolute]{textpos}
\usepackage[it]{subfigure}
\usepackage{graphicx}
\usepackage{cmbright}
%\usepackage[default]{cantarell}
%\usepackage{avant}
%\usepackage[math]{iwona}
\usepackage[math]{kurier}
\usepackage[T1]{fontenc}

\usepackage{diagbox}

%% add your packages here
\usepackage{hyperref}
% for random text
\usepackage{lipsum}
\usepackage[english]{babel}
\usepackage[pangram]{blindtext}


\colorlet{backgroundcolor}{blue!10}

 % Title, Author, Institute
\title{FLIP01 Final Assessment}
\author{Cong Ma}
\institute{$^1$ QingDao Technological University, China \\
}
%\titlegraphic{logos/tulip-logo.eps}

%Choose Layout
\usetheme{Wave}

%\definebackgroundstyle{samplebackgroundstyle}{
%\draw[inner sep=0pt, line width=0pt, color=red, fill=backgroundcolor!30!black]
%(bottomleft) rectangle (topright);
%}
%
%\colorlet{backgroundcolor}{blue!10}

\begin{document}


\colorlet{blocktitlebgcolor}{blue!23}

 % Title block with title, author, logo, etc.
\maketitle

\begin{columns}
 % FIRST column
\column{0.5}% Width set relative to text width

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
 %\block{Main Objectives}{
%  	      	\begin{enumerate}
%  	      	\item Formalise research problem by extending \emph{outlying aspects mining}
%  	      	\item Proposed \emph{GOAM} algorithm is to solve research problem
%  	      	\item Utilise pruning strategies to reduce time complexity
%  	      	\end{enumerate}
%%  	      \end{minipage}
%}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{MODEL:BERT}{
      % Many real world applications call for one important function
    \begin{itemize}	

    \item
    A robust method to optimize the best pre training.

    \end{itemize}

  	\begin{description}
  	%\item[Outlying Aspects Mining] aims to identify a subspace
    %The task of \emph{Outlying Aspects Mining}

    \item
    The essence of best is to run self supervised learning method on the basis of massive corpus
    to provide a good feature representation for word learning. The so-called self supervised
    learning refers to the supervised learning on the data without manual annotation. In future
    specific NLP tasks, we can directly use the feature representation of Bert as the word
    embedding feature of the task. So what Bert provides is a model for other task transfer
    learning, which can be used as feature extractor after task fine-tuning or fixed.

  	\end{description}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{BERT:MLM Shortcoming}{
\begin{itemize}
    %\emph{Group Outlying Aspects Mining}

    \item
     MLM(Masked Language Model) means that during training, some words are masked from the input, and then the words are predicted by the context of the input,

    \item
      In Bert's experiment, 15\% of wordpiece tokens were randomly masked out.
    When training the model, a sentence will be fed to the model many times
    for parameter learning. However, Google does not mask these words every time.
    Instead, it will directly replace them with [mask] 80\% of the time, replace
    them with any other words 10\% of the time, and retain the original
    token 10\% of the time.

    \item
    For example: \\
     80\% : "my dog is hairy" -> my dog is [mask]\\
     10\% : "my dog is hairy" -> my dog is  apple\\
     10\% : "my dog is hairy" -> my dog is  hairy
\end{itemize}

\vspace{.5cm}

%数字和符号之间加 " \ "
\center

\vspace{.2cm}


}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

%\note{Note with default behavior}

%\note[targetoffsetx=12cm, targetoffsety=-1cm, angle=20, rotate=25]
%{Note \\ offset and rotated}

 % First column - second block


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{BERT:NSP}{


%\begin{tikzfigure}%[Overall architecture of \emph{GOAM} algorithm]
%    \missingfigure[figcolor=white]{Testing figcolor}
%\end{tikzfigure}

%  where $G_q$ is the query group,
%  $n$ is the number of compare groups,
%  and $h_{k_s}$ is the histogram representation of $G_k$ in the subspace $s$.

\begin{description}

  	\item
   The task of Next Sentence Prediction (NSP) is to determine whether sentence B
   is the following part of sentence a. If yes, output 'isnext', otherwise
   output 'notnext'. \\The training data is generated by randomly extracting
   two consecutive sentences from the parallel corpus, 50\% of which keep
   the extracted two sentences, which conform to the isnext relationship,
   and the other 50\% of the second sentences are randomly extracted from the
   expectation, whose relationship is not next.

\end{description}


}

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% SECOND column
\column{0.5}
 %Second column with first block's top edge aligned with with previous column's top.

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
% Second column - first block


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\block{roBERTa:An improved BERT algorithm}{
   \begin{itemize}	

    \item
    Roberta makes the following optimization for Bert.

    \begin{itemize}
    \item
    Use bigger batch and more data to make the model train longer.
    \item
    Removed the NSP (next sense prediction) task.
    \item
    Using dynamic masking
    \end{itemize}
   \end{itemize}
  	
}

\block{Static masking vs dynamic masking}
{
\begin{description}

  	\item
    Static making: during data preprocessing, the mask matrix has been generated.
    Each sample will be randomly masked only once, and each epoch is the same.

    \item
    Dynamic masking: every time a sequence is input to the model,
    a new maks method is generated. That is, the mask is not generated
    during preprocessing, but dynamically generated when the input is provided
    to the model.
\end{description}
}


\block{Remove NSP}
{
\begin{description}
  	\item
    Roberta experimented with four methods and finally came to a conclusion:
    Eliminating NSP tasks has no effect on our model optimization.
\end{description}
}

\block{Summary}
{
\begin{description}
  	
  \item
  *Change the mask strategy dynamically, copy 10 copies of data, and
  then do random mask uniformly.
  \item
  *The peak value of learning rate and the number of warm up
  update steps are adjusted.
  \item
  *Training on longer sequences: do not truncate the sequences,
  use full length sequences.
  \item
  *Compared with Bert model, Roberta model uses more data, eliminates 
  unnecessary operations and achieves higher accuracy.
\end{description}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% Second column - second block
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%


% Bottomblock
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
\colorlet{notebgcolor}{blue!20}
\colorlet{notefrcolor}{blue!20}


%\note[targetoffsetx=8cm, targetoffsety=-10cm,rotate=0,angle=180,radius=8cm,width=.46\textwidth,innersep=.1cm]{
%Acknowledgement
%}

%\block[titlewidthscale=0.9, bodywidthscale=0.9]
%{Acknowledgement}{
%}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

\end{columns}


%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%
%[titleleft, titleoffsetx=2em, titleoffsety=1em, bodyoffsetx=2em,%
%roundedcorners=10, linewidth=0mm, titlewidthscale=0.7,%
%bodywidthscale=0.9, titlecenter]

%\colorlet{noteframecolor}{blue!20}
\colorlet{notebgcolor}{blue!20}
\colorlet{notefrcolor}{blue!20}
%           左右                                            上下
\note[targetoffsetx=-13cm, targetoffsety=-15cm,rotate=0,angle=180,radius=8cm,width=.96\textwidth,innersep=.4cm]
{
\begin{minipage}{0.3\linewidth}
\centering
\includegraphics[width=24cm]{logos/tulip-wordmark.eps}
\end{minipage}
\begin{minipage}{0.7\linewidth}
{
\center
 Thanks for watching  2021/1/16
}
\end{minipage}
}
%%%%%%%%%% -------------------------------------------------------------------- %%%%%%%%%%

\end{document}

%\endinput
%%
%% End of file `tikzposter-template.tex'.

